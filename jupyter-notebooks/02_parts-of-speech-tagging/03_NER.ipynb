{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c6f2c6-b8f7-4bda-9721-b02b3c890d6c",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "NER seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as person names, organisations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdb798c-1d3c-4681-a617-ce6107d36c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f304ebca-3fd7-40ea-a33e-7e172aaecf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display basic entity info:\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43f3adf-bc66-4a67-aaec-f4f35fb2afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington - GPE - Countries, cities, states\n",
      "DC - GPE - Countries, cities, states\n",
      "next May - DATE - Absolute or relative dates or periods\n",
      "the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514918ea-c54f-48be-aeae-fb48d1e3ec58",
   "metadata": {},
   "source": [
    "Here we see tokens combine to form the entities `Washington`, `DC`, `next May` and `the Washington Monument`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06548b5-f23e-467d-882e-93ea7abc88ca",
   "metadata": {},
   "source": [
    "## Entity annotations\n",
    "`Doc.ents` are token spans with their own set of annotations.\n",
    "<table>\n",
    "<tr><td>`ent.text`</td><td>The original entity text</td></tr>\n",
    "<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>\n",
    "<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>\n",
    "<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54f9200-5c40-4951-811b-20548f49412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 dollars 4 6 20 31 MONEY\n",
      "Microsoft 11 12 53 62 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7f3ee-9b2b-409c-b04a-280d458fa011",
   "metadata": {},
   "source": [
    "## NER Tags\n",
    "Tags are accessible through the `.label_` property of an entity.\n",
    "<table>\n",
    "<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n",
    "<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n",
    "<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n",
    "<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n",
    "<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n",
    "<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n",
    "<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n",
    "<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n",
    "<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n",
    "<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n",
    "<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n",
    "<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n",
    "<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n",
    "<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n",
    "<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n",
    "<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n",
    "<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n",
    "<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n",
    "<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50550de0-157f-44a0-ab97-fcbc03bddd19",
   "metadata": {},
   "source": [
    "___\n",
    "## Adding a Named Entity to a Span\n",
    "Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e98fa7-b584-4c4c-bb72-87e2d5033200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa24c4-45ce-4867-9257-1be27de3902e",
   "metadata": {},
   "source": [
    "<font color=green>Right now, spaCy does not recognize \"Tesla\" as a company.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e570e9-a19f-4314-b963-529dcef17ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Get the hash value of the ORG entity label\n",
    "ORG = doc.vocab.strings[u'ORG']  \n",
    "\n",
    "# Create a Span for the new entity\n",
    "new_ent = Span(doc, 0, 1, label=ORG)\n",
    "\n",
    "# Add the entity to the existing Doc object\n",
    "doc.ents = list(doc.ents) + [new_ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603eb50-07d2-4eda-974c-30930df5ae82",
   "metadata": {},
   "source": [
    "<font color=green>In the code above, the arguments passed to `Span()` are:</font>\n",
    "-  `doc` - the name of the Doc object\n",
    "-  `0` - the *start* index position of the span\n",
    "-  `1` - the *stop* index position (exclusive)\n",
    "-  `label=ORG` - the label assigned to our entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54ec444-6044-4e45-a761-ca32a8d96d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla - ORG - Companies, agencies, institutions, etc.\n",
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b1068-e90b-4364-b030-73d620958412",
   "metadata": {},
   "source": [
    "___\n",
    "## Adding Named Entities to All Matching Spans\n",
    "What if we want to tag *all* occurrences of \"Tesla\"? In this section we show how to use the `PhraseMatcher` to identify a series of spans in the Doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40f0f38-a5ff-4771-ac8c-ff07d5a83726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first - ORDINAL - \"first\", \"second\", etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Our company plans to introduce a new vacuum cleaner. '\n",
    "          u'If successful, the vacuum cleaner will be our first product.')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73328dde-2570-4368-83c7-1360a3966729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PhraseMatcher and create a matcher object:\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d765741-34aa-4747-ae4a-091f0a5e388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the desired phrase patterns:\n",
    "phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332bc857-15b3-4c11-b4f5-b20666cd28ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2689272359382549672, 7, 9), (2689272359382549672, 14, 16)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the patterns to our matcher object:\n",
    "matcher.add('newproduct', None, *phrase_patterns)\n",
    "\n",
    "# Apply the matcher to our Doc object:\n",
    "matches = matcher(doc)\n",
    "\n",
    "# See what matches occur:\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2c7ab0-8d59-4f11-8341-1b4adfcbf517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create Spans from each match, and create named entities from them:\n",
    "from spacy.tokens import Span\n",
    "\n",
    "PROD = doc.vocab.strings[u'PRODUCT']\n",
    "\n",
    "new_ents = [Span(doc, match[1], match[2], label=PROD) for match in matches]\n",
    "\n",
    "doc.ents = list(doc.ents) + new_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e95f14-2354-4a10-ba75-589eed609d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n",
      "vacuum cleaner - PRODUCT - Objects, vehicles, foods, etc. (not services)\n",
      "first - ORDINAL - \"first\", \"second\", etc.\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fa662-1ead-481f-90a1-489cc934b30c",
   "metadata": {},
   "source": [
    "___\n",
    "## Counting Entities\n",
    "While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8fffd5-63fb-42bc-9fa6-1cd85cd96800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.50 - MONEY - Monetary values, including unit\n",
      "five dollars - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6af586e-c7f8-4e75-b9f7-eb73ddf60b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ent for ent in doc.ents if ent.label_=='MONEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211e8de-36f7-4f8a-987b-ac3e40e5e987",
   "metadata": {},
   "source": [
    "___\n",
    "## Noun Chunks\n",
    "`Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>\n",
    "Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa04b3-d0b9-43b5-8c53-c202049bd8ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `noun_chunks` components:\n",
    "<table>\n",
    "<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>\n",
    "<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>\n",
    "<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>\n",
    "<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c37992-d962-4b13-9272-a3cdeeb2b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars - cars - nsubj - shift\n",
      "insurance liability - liability - dobj - shift\n",
      "manufacturers - manufacturers - pobj - toward\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text + ' - ' + chunk.root.text + ' - ' + chunk.root.dep_ + ' - ' + chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a7cd0-6f9e-478a-9230-2436b1022035",
   "metadata": {},
   "source": [
    "For more on **noun_chunks** visit https://spacy.io/usage/linguistic-features#noun-chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a57e7-94b3-4bbb-9cac-b72ef20fb7d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9f9e34-385c-4564-aff6-6fec718e0e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.50 - MONEY - Monetary values, including unit\n",
      "five dollars - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n",
    "\n",
    "show_ents(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
